# -*- coding: utf-8 -*-
"""Dicoding_Proyek Rekomendasi Ponsel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQJD3GLdYEKzW8MkQAuHGr3S0EROqPAX

# **Proyek: Sistem Rekomendasi Ponsel**

# **Latar Belakang**

Dalam era digital yang berkembang pesat, sistem rekomendasi telah menjadi elemen penting, termasuk dalam industri ponsel. Dengan banyaknya model ponsel yang tersedia, konsumen sering mengalami kesulitan untuk memilih produk yang sesuai dengan kebutuhan mereka. Sistem rekomendasi ponsel dirancang untuk mengatasi permasalahan ini dengan menyediakan saran yang relevan berdasarkan preferensi dan kebutuhan individu. Salah satu alasan utama pentingnya sistem ini adalah tingginya volume informasi. Setiap tahun, ratusan model ponsel dirilis dengan spesifikasi, fitur, dan harga yang bervariasi, sehingga sulit bagi konsumen untuk membandingkan semuanya. Selain itu, kebutuhan personalisasi juga menjadi alasan utama, karena setiap konsumen memiliki preferensi unik, seperti kamera berkualitas tinggi, performa gaming, daya tahan baterai, atau harga yang terjangkau. Sistem ini juga membantu menghemat waktu, memberikan rekomendasi cepat tanpa perlu pencarian manual yang memakan waktu.

Sistem rekomendasi ini memberikan manfaat besar, baik bagi produsen maupun konsumen. Bagi produsen, sistem ini dapat meningkatkan penjualan dengan menarik lebih banyak pelanggan melalui rekomendasi personal. Selain itu, analisis data rekomendasi membantu produsen memahami preferensi pasar, memungkinkan mereka mengembangkan produk yang lebih sesuai dengan kebutuhan konsumen. Pengalaman belanja yang dipersonalisasi juga membantu membangun loyalitas merek. Di sisi lain, konsumen mendapatkan kemudahan dalam membuat keputusan dengan rekomendasi yang spesifik dan relevan. Sistem ini juga menghemat waktu konsumen dalam menemukan ponsel yang sesuai dan memberikan pengalaman belanja yang lebih memuaskan. Dengan demikian, sistem rekomendasi berbasis content-based filtering dan collaborative filtering menjadi solusi cerdas untuk memenuhi kebutuhan pasar dan memberikan pengalaman yang lebih baik bagi semua pihak yang terlibat.

Pendekatan yang digunakan dalam sistem rekomendasi melibatkan dua metode utama, yaitu content-based filtering dan collaborative filtering. Content-based filtering bekerja dengan menganalisis karakteristik ponsel, seperti prosesor, RAM, kapasitas penyimpanan, kamera, dan harga, kemudian mencocokkannya dengan preferensi pengguna. Misalnya, jika pengguna menginginkan ponsel dengan kamera berkualitas tinggi, sistem akan merekomendasikan model dengan fitur serupa. Sementara itu, collaborative filtering memanfaatkan data historis dari pengguna lain. Jika banyak pengguna dengan preferensi serupa menyukai model tertentu, sistem akan merekomendasikan model tersebut kepada pengguna baru dengan profil yang mirip.

Penerapan metode Content-Based Filtering dalam sistem rekomendasi ponsel dapat memberikan rekomendasi yang lebih tepat dengan mempertimbangkan preferensi spesifik pengguna, seperti kapasitas baterai, prosesor, dan ukuran layar. Suatu penelitian mengulas penerapan model berbasis konten yang menggunakan berbagai faktor untuk menyesuaikan rekomendasi produk (termasuk ponsel) dengan preferensi pengguna [1]. Pendekatan lain yang lebih canggih mempertimbangkan konteks pengguna, seperti waktu dan lokasi, untuk memberikan rekomendasi yang lebih relevan dan personal [2]. Sementara itu, Collaborative Filtering digunakan dalam sistem rekomendasi ponsel dengan memanfaatkan data dari pengguna lain yang memiliki preferensi serupa, yang efektif untuk menemukan produk baru berdasarkan pola perilaku kolektif [3]. Metode-metode ini, meskipun berbeda, memberikan kontribusi besar dalam meningkatkan relevansi rekomendasi ponsel untuk pengguna individu.

Referensi :\
[1] [Development of Content-Based Filtering Model for Recommendation System Using Multiple Factors related to object Preference](https://ieeexplore.ieee.org/document/10639015)\
[2] [Context-Aware Recommender System based on Content Filtering](https://ieeexplore.ieee.org/document/9620478)\
[2] [A Survey of Collaborative Filtering-Based Recommender Systems for Mobile Internet Applications](https://ieeexplore.ieee.org/document/7479487)

# **Business Understanding**

## **Problem Statements**

- Pengguna mengalami kesulitan dalam memilih ponsel yang sesuai dengan kebutuhan pribadi mereka.
- Terlalu banyak pilihan ponsel dengan spesifikasi yang serupa, yang membuat pengguna bingung dalam menentukan pilihan yang tepat.
- Sistem rekomendasi saat ini tidak cukup personal dan relevan dengan preferensi individu pengguna.
- Minimnya data interaksi pengguna dengan ponsel sebelumnya menyulitkan sistem dalam memberikan rekomendasi yang akurat (terutama pada kasus cold start).
- Tingkat konversi penjualan ponsel bisa lebih tinggi dengan sistem rekomendasi yang lebih tepat dan didasarkan pada data preferensi pengguna.

## **Goals**

- Meningkatkan pengalaman pengguna dengan memberikan rekomendasi ponsel yang sesuai dengan preferensi mereka (berdasarkan fitur dan spesifikasi produk yang relevan). Dengan metode Content-Based Filtering, rekomendasi disesuaikan dengan preferensi spesifik pengguna terkait fitur produk seperti ukuran layar, kapasitas baterai, dan kualitas kamera.
- Menyaring ponsel yang memiliki fitur serupa dengan yang telah dipilih oleh pengguna sebelumnya untuk mengurangi kebingungan dalam memilih produk. Dengan menggunakan Content-Based Filtering, sistem akan menyoroti ponsel yang memiliki karakteristik atau spesifikasi yang mirip dengan produk yang sebelumnya dipilih atau disukai oleh pengguna.
- Memberikan rekomendasi yang lebih personal dan relevan dengan memanfaatkan data preferensi pengguna secara lebih efektif. Collaborative Filtering akan menganalisis data perilaku pengguna lain yang memiliki preferensi serupa, sehingga bisa memberikan rekomendasi yang lebih terpersonalisasi untuk setiap pengguna.
- Mengatasi masalah cold start dengan memanfaatkan metode Content-Based Filtering yang tidak memerlukan data historis pengguna untuk memberikan rekomendasi yang relevan. Dengan Content-Based Filtering, meskipun data interaksi pengguna terbatas, sistem tetap dapat memberikan rekomendasi berdasarkan atribut ponsel yang relevan dengan kebutuhan pengguna.
- Meningkatkan konversi penjualan ponsel dengan menyarankan model yang lebih sesuai dengan kebutuhan pelanggan, yang akhirnya mendorong pembelian yang lebih tepat. Sistem rekomendasi yang lebih akurat dan relevan, berkat Content-Based Filtering dan Collaborative Filtering, berpotensi meningkatkan kepuasan pengguna dan mendorong keputusan pembelian yang lebih baik.

## **Solutions**

- Menggunakan pendekatan content based filtering berbasis cosine similarity.
- Menggunakan pendekatan collaborative filtering berbasis Singular Value Decomposition (SVD).
- Melakukan hyperparameter tuning pada metode content based filtering untuk mengetahui fitur apa yang paling relevan dengan untuk memberikan rekomendasi dan berapa jumlah rekomendasi yang optimal.
- Melakukan hyperparameter tuning pada metode collaborative filtering untuk mendapatkan representasi user-item yang paling relevan berdasarkan faktor laten dan berapa jumlah rekomendasi yang optimal.

# **Datasets**

Dataset Cellphones Recommendations terdiri dari 3 berkas, yaitu **cellphone data.csv** yang berisi 33 data tentang ponsel paling populer di AS pada tahun 2022, **cellphones users.csv** yang berisi 99 data pengguna, dan **cellphones ratings.csv** yang berisi 990 data rating dari pengguna untuk ponsel terkait. Rating diperoleh dengan melakukan survei pada Mechanical Turk. Setiap peserta diberikan 10 ponsel acak, dan ia diminta untuk menunjukkan seberapa besar kemungkinan ia akan membeli setiap ponsel dengan harga yang diberikan, pada skala dari 1 (sangat tidak mungkin) hingga 10 (sangat mungkin). Setiap peserta diminta untuk menambahkan informasi pribadi: usia, jenis kelamin, dan pekerjaan.

Atribut pada **cellphones data.csv**
* **cellphone_id :** id setiap ponsel
* **brand :** merek ponsel
* **model :** tipe ponsel
* **operating system :** sistem operasi (Android atau iOS)
* **internal memory :** ukuran memori (GB)
* **RAM :** RAM (GB)
* **performance :** peringkat kinerja (AnTuTu)
* **main camera :** resolusi kamera utama (MP)
* **selfie camera :** resolusi kamera depan (MP)
* **battery size :** ukuran baterai (mAh)
* **screen size :** ukuran layar (inch)
* **weight :** berat (gr)
* **price :** harga dari Amazon dan Best-Buy (pada Agustus 22)
* **release date :** tanggal rilis

Atribut pada **cellphones users.csv**
* **user_id :** id pengguna
* **age :** umur pengguna
* **gender :** gender pengguna
* **occupation :** pekerjaan pengguna

Atribut pada **cellphones ratings.csv**
* **user_id :** id pengguna
* **cellphones_id :** id ponsel
* **rating :** rating dari pengguna pada ponsel (1-10)

[Kaggle](https://www.kaggle.com/datasets/meirnizri/cellphones-recommendations)

# **Import Library**
"""

import kagglehub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.linalg import svds

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 300)

"""# **Load Data**"""

# Download latest version
path = kagglehub.dataset_download("meirnizri/cellphones-recommendations")
print("Path to dataset files:", path)

# Daftar file pada path
os.listdir(path)

# Read semua data csv
df_rating = pd.read_csv(path + '/cellphones ratings.csv')
df_data = pd.read_csv(path + '/cellphones data.csv')
df_user = pd.read_csv(path + '/cellphones users.csv')

"""# **Exploratory Data Analysis**

## **Rating**
"""

# Cek 5 data pertama pada dataset rating
df_rating.head()

# Cek jumlah dan tipe data pada dataset rating
df_rating.info()

# Cek jumlah user, jumlah ponsel, dan skala rating
print('Jumlah user   :', df_rating['user_id'].nunique())
print('Jumlah ponsel :', df_rating['cellphone_id'].nunique())
print('Rating        :', df_rating['rating'].unique())

df_rating[df_rating['rating'] == 18]

# Mengganti nilai rating 18 menjadi 10
df_rating.loc[df_rating['rating'] == 18, 'rating'] = 10

# Cek jumlah user, jumlah ponsel, dan skala rating
print('Jumlah user   :', df_rating['user_id'].nunique())
print('Jumlah ponsel :', df_rating['cellphone_id'].nunique())
print('Rating        :', df_rating['rating'].unique())

# Cek missing value pada dataset rating
null = df_rating.isnull().sum()
null.name = 'Null'
null

# Cek apakah dataset rating memiliki record yang duplikat
df_rating.duplicated().sum()

# Jumlah rating dari setiap user
rat_count = df_rating['user_id'].value_counts()
print('Banyak rating setiap user :', rat_count.unique())

# Cek apakah setiap user pernah memberikan rating lebih dari sekali pada produk yang sama
df_rating[['user_id', 'cellphone_id']].duplicated().sum()

"""**Kondisi Dataset Rating**
* Terdapat 990 data rating dari user untuk ponsel
* Terdapat 99 user yang memberikan rating dan 33 ponsel yang diberikan rating dengan skala 1-10
* Terdapat outlier pada 1 data di mana rating 18 di mana seharusnya 1-10
* Tidak terdapat missing value
* Tidak terdapat record yang duplikat
* Setiap user memberikan rating pada 10 ponsel berbeda

## **Data**
"""

# Cek 5 data pertama pada dataset data
df_data.head()

# Cek jumlah dan tipe data pada dataset data
df_data.info()

# Cek missing value pada dataset data
null = df_data.isnull().sum()
null.name = 'Null'
null

# Cek apakah dataset data memiliki record yang duplikat
df_data.duplicated().sum()

# Jumlah model dari setiap brand ponsel
model = df_data.groupby(['operating system', 'brand'])['model'].count()
model['Total', ''] = model.sum()
model

"""**Kondisi Dataset Data**
* Terdapat 33 data spesifikasi ponsel yang unik
* Tidak terdapat missing value
* Tidak terdapat record yang duplikat

# **Data Preprocessing**

## **Content Based Filtering**

### **Label Generation**

Proses mendapatkan label untuk content based filtering. Label pada content based filtering merupakan pasangan dari setiap **user_id** dengan **cellphone_id** yang memiliki minimal 2 buah rating di mana rating >= 8 (asumsi rating ponsel yang disukai). Setiap user_id minimal memiliki 2 buah rating karena salah satu **cellphone_id** yang bersesuaian akan digunakan untuk mendapatkan rekomendasi sehingga **cellphone_id** lainnya dapat digunakan untuk mengevaluasi relevansi rekomendasi tersebut.
"""

# Filter rating, simpan data dengan rating >= 8 (asumsi rating ponsel yang disukai)
df_label_con = df_rating[df_rating['rating'] >= 8]
# Urutkan rating secara descending (mulai dari tertinggi)
df_label_con = df_label_con.sort_values('rating', ascending=False)
# Group rating berdasarkan user_id dan agregasikan semua nilai cellphone_id ke dalam list
df_label_con = df_label_con.groupby('user_id')['cellphone_id'].apply(list)
# Filter user_id, simpan user_id dengan jumlah cellphone_id > 1
df_label_con = df_label_con.apply(lambda x: x if len(x) > 1 else None).dropna()
df_label_con.head(5)

# Panjang ground truth setiap user
length = df_label_con.apply(lambda x: len(x))
length.name = 'Length'
# Jumlah ground truth dengan panjang tertentu
count = length.value_counts()
count['Total'] = count.sum()
count

"""### **Feature Engineering**

Untuk mendapatkan fitur yang lebih optimal seperti **year** (tahun) dibandingkan dengan **release date** (tanggal rilis)
"""

df_data_process = df_data.copy()
# Mengekstrak tahun dari release date
df_data_process['year'] = pd.to_datetime(df_data_process['release date'], dayfirst=True).dt.year.astype('object')
# Drop atribut release date
drop_cols = ['cellphone_id', 'model', 'release date']
df_data_process = df_data_process.drop(drop_cols, axis=1)
df_data_process.sample(5)

"""### **Split Attributes**

Memisahkan atribut numerik dan kategorikal karena membutuhkan penanganan yang berbeda
"""

# Split data numerik dan kategorikal
num_cols = df_data_process.select_dtypes(np.number).columns
cat_cols = df_data_process.select_dtypes('object').columns
df_num, df_cat = df_data_process[num_cols].copy(), df_data_process[cat_cols].copy()

"""### **One Hot Encoding**

Data kategori diubah menjadi numerik agar dapat dilakukan modelling
"""

# One hot encoding
enc = OneHotEncoder(sparse_output=False, drop='first')
df_cat = pd.DataFrame(enc.fit_transform(df_cat))
df_cat.columns = enc.get_feature_names_out()
df_cat.head()

"""### **Feature Scaling**

Data numerik dinormalisasi untuk menyamakan rentang nilai menjadi antara 0-1
"""

# Feature scaling menggunakan MinMaxScaler
sc = MinMaxScaler()
df_num = pd.DataFrame(sc.fit_transform(df_num))
df_num.columns = num_cols
df_num.head()

"""### **Combine Attribute**

Atribut numerik dan kategori digabungkan untuk membentuk satu kesatuan fitur
"""

# Nama ponsel
phone_name = df_data['brand'] + ' ' + df_data['model']
# ID ponsel
phone_id = df_data['cellphone_id']
# Dictionary yang memetakan id ke name
id_to_name = {id: name for id, name in zip(phone_id, phone_name)}
# Dictionary yang memetakan name ke id
name_to_id = {name: id for id, name in zip(phone_id, phone_name)}

# Gabungkan dengan data numerik dan kategorikal
df_data_process = pd.concat([df_num, df_cat], axis=1)
df_data_process.index = phone_id
df_data_process.rename(index=id_to_name).sample(5)

"""## **Collaborative Filtering**

### **Train-Test Split**

Agar performa model lebih valid maka model diuji pada data yang berbeda dari data training
"""

# Split dataset menjadi dataset train dan dataset test
df_rating_train, df_rating_test = train_test_split(df_rating, test_size = 0.5, random_state = 42, stratify=df_rating['user_id'])

# Reset index
df_rating_train = df_rating_train.reset_index(drop=True)
df_rating_test = df_rating_test.reset_index(drop=True)

"""### **Label Generation**

Proses mendapatkan label untuk collaborative filtering. Label pada collaborative filtering merupakan pasangan dari setiap user_id dengan cellphone_id yang memiliki rating >= 8 (asumsi rating ponsel yang disukai)
"""

# Filter rating, simpan data dengan rating >= 8 (asumsi rating ponsel yang disukai)
df_label_col = df_rating_test[df_rating_test['rating'] >= 8]
# Urutkan rating secara descending (mulai dari tertinggi)
df_label_col = df_label_col.sort_values('rating', ascending=False)
# Group rating berdasarkan user_id dan agregasikan semua nilai cellphone_id ke dalam list
df_label_col = df_label_col.groupby('user_id')['cellphone_id'].apply(list)

df_label_col

# Panjang ground truth setiap user
length = df_label_col.apply(lambda x: len(x))
length.name = 'Length'
# Jumlah ground truth dengan panjang tertentu
count = length.value_counts()
count['Total'] = count.sum()
count

"""### **Pivot Table**

Membuat pivot table yang berisi rating dari setiap user untuk setiap ponsel yang ada
"""

# Pivot table untuk membentuk matriks user-item berdasarkan data training
df_pivot_train = df_rating_train.pivot_table(index='user_id', columns='cellphone_id', values='rating')
# Pivot table untuk membentuk matriks user-item berdasarkan data test
df_pivot_test = df_rating_test.pivot_table(index='user_id', columns='cellphone_id', values='rating')

print(df_pivot_train.shape)
df_pivot_train.sample(5)

print(df_pivot_test.shape)
df_pivot_train.sample(5)

"""NaN merepresentasikan bahwa user tersebut tidak memberikan rating pada cellphone_id yang bersesuaian

### **Fill Missing Value**

Mengisi nilai NaN dari pivot table yang diakibatkan oleh tidak adanya rating yang diberikan user pada ponsel tertentu. Ponsel yang tidak diberi rating akan diberi nilai 0.
"""

# Isi NaN dengan 0
df_pivot_train = df_pivot_train.fillna(0)
df_pivot_test = df_pivot_test.fillna(0)

df_pivot_train.head(5)

df_pivot_test.head(5)

# Hitung banyak rating dari setiap user_id
(df_pivot_train != 0).sum(axis=1).value_counts()

# Hitung banyak rating dari setiap user_id
(df_pivot_test != 0).sum(axis=1).value_counts()

"""Setiap user memberikan rating pada 7 produk berbeda pada data training dan 3 produk berbeda pada data test

# **Modelling**

## **Content Based Filtering**

### **Cosine Similarity**
"""

# Menghitung cosine similarity
cosine = cosine_similarity(df_data_process)
# Membuat dataframe dari variabel cosine dengan baris dan kolom berupa id ponsel
df_cosine = pd.DataFrame(cosine, index=phone_id, columns=phone_id)
print('Ukuran matriks similaritas:', df_cosine.shape)

# Melihat matriks similaritas pada setiap ponsel
df_cosine.rename(index=id_to_name, columns=id_to_name).sample(5, axis=1).sample(10, axis=0)

"""### **Evaluation**"""

class CONTENT():
  def __init__(self, cols=None):
    self.cols = cols

  def fit(self, df_data):
    # Copy dataframe
    df_data_process = df_data.copy()

    # Mengekstrak tahun dari release date
    df_data_process['year'] = pd.to_datetime(df_data_process['release date'], dayfirst=True).dt.year.astype('object')
    df_data_process = df_data_process.drop(['cellphone_id', 'model', 'release date'], axis=1)

    # Select column
    if self.cols != None:
      df_data_process = df_data_process[self.cols]

    # Split atribut numerik dan kategorikal
    num_cols = df_data_process.select_dtypes(np.number).columns
    cat_cols = df_data_process.select_dtypes('object').columns
    df_num, df_cat = df_data_process[num_cols].copy(), df_data_process[cat_cols].copy()

    # One hot encoding
    if len(cat_cols) > 0:
      enc = OneHotEncoder(sparse_output=False, drop='first')
      df_cat = pd.DataFrame(enc.fit_transform(df_cat))
      df_cat.columns = enc.get_feature_names_out()

    # Feature scaling menggunakan MinMaxScaler
    if len(num_cols) > 0:
      sc = MinMaxScaler()
      df_num = pd.DataFrame(sc.fit_transform(df_num))
      df_num.columns = num_cols

    # Gabungkan data numerik dan kategorikal
    df_data_process = pd.concat([df_num, df_cat], axis=1)
    df_data_process.index = phone_id

    # Menghitung cosine similarity
    cosine = cosine_similarity(df_data_process)
    self.df_cosine = pd.DataFrame(cosine, index=phone_id, columns=phone_id)

  def recommendations(self, phone_id, k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    index = self.df_cosine.loc[:, phone_id].to_numpy().argpartition(range(-1, -k, -1))
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = self.df_cosine.columns[index[-1:-(k+2):-1]]
    # Drop phone_id agar id ponsel yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(phone_id, errors='ignore')
    return closest

def precision_at_k(recommended, relevant):
  hits = len(set(recommended) & set(relevant))
  precision = hits / len(recommended)
  return precision

def recall_at_k(recommended, relevant):
  hits = len(set(recommended) & set(relevant))
  recall = hits / len(relevant)
  return recall

def f1_at_k(recommended, relevant):
  precision = precision_at_k(recommended, relevant)
  recall = recall_at_k(recommended, relevant)
  if precision + recall == 0:
    f1 = 0
  else:
    f1 = 2 * (precision * recall) / (precision + recall)
  return f1

"""#### **Latent Factor**"""

cols ={
    'All': None,
    'No Performance': ['main camera', 'selfie camera', 'battery size', 'screen size', 'weight', 'price', 'brand', 'operating system', 'year'],
    'No Camera': ['internal memory', 'RAM', 'performance', 'battery size', 'screen size', 'weight', 'price', 'brand', 'operating system', 'year'],
    'No Battery': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'screen size', 'weight', 'price', 'brand', 'operating system', 'year'],
    'No Screen': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'battery size', 'weight', 'price', 'brand', 'operating system', 'year'],
    'No Weight': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'battery size', 'screen size', 'price', 'brand', 'operating system', 'year'],
    'No Price': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'battery size', 'screen size', 'weight', 'brand', 'operating system', 'year'],
    'No Brand': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'battery size', 'screen size', 'weight', 'price', 'operating system', 'year'],
    'No OS': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'battery size', 'screen size', 'weight', 'price', 'brand', 'year'],
    'No Year': ['internal memory', 'RAM', 'performance', 'main camera', 'selfie camera', 'battery size', 'screen size', 'weight', 'price', 'brand', 'operating system']
}

results = {}

# Jumlah rekomendasi
n = 5

# Evaluasi setiap parameter dalam dictionary cols
for key, value in cols.items():

  # Latih model content based filtering
  cbf = CONTENT(value)
  cbf.fit(df_data)

  precision_scores = []
  recall_scores = []
  f1_scores = []

  # Evaluasi setiap user
  for cellphone_id in df_label_con:

    # Evaluasi setiap relevant item
    for i in range(len(cellphone_id)):

      # Rekomendasi ponsel berdasarkan phone_id sebanyak n
      recom_id = cbf.recommendations(cellphone_id[i], n)
      # Ponsel yang relevan bagi pengguna
      relevants = [id for id in cellphone_id if id != cellphone_id[i]]
      # Hitung precision
      precision = precision_at_k(recom_id, relevants)
      # Hitung recall
      recall = recall_at_k(recom_id, relevants)
      # Hitung f1
      f1 = f1_at_k(recom_id, relevants)
      # Simpan precision
      precision_scores.append(precision)
      # Simpan recall
      recall_scores.append(recall)
      # Simpan f1
      f1_scores.append(f1)

  # Hasil rata-rata
  avg_precision = np.mean(precision_scores)
  avg_recall = np.mean(recall_scores)
  avg_f1 = np.mean(f1_scores)

  # Simpan hasil evaluasi untuk setiap parameter (kolom)
  results[key] = {
      'precision': avg_precision,
      'recall': avg_recall,
      'f1': avg_f1
  }

# Hasil evaluasi fitur pada content based filtering
sort_results = pd.DataFrame(results).T.apply(lambda x: round(x, 4)*100).sort_values('f1', ascending=False)
sort_results

# Grafik hasil evaluasi fitur pada content based filtering
sort_results['f1'].sort_values().plot(kind='barh',
                                      title='Feature Evaluation (Content Based Filtering)',
                                      xlim=(14.5, 17.5),
                                      xlabel='F1@K (%)',
                                      ylabel='Feature')
plt.show()

"""Berdasarkan hasil diatas tampak bahwa fitur paling optimal pada content based filtering untuk menentukan rekomendasi ponsel pada pengguna adalah seluruh fitur kecuali ukuran layar ("No Screen"). Hal ini berarti, berdasarkan rating yang diberikan oleh pengguna dalam database, secara umum pengguna kurang mempertimbangkan ukuran layar dalam memilih dan menyukai suatu ponsel. Sebaliknya pengguna sangat memperhatikan merek dalam memilih ponsel. Hal ini ditunjukkan oleh performa yang rendah dari model content based filtering ketika fitur **brand** tidak dipertimbangkan dalam pemberian rekomendasi.

#### **N recommendation**
"""

results = {}

# Jumlah rekomendasi (n) yang dievaluasi
list_n = [3, 4, 5, 6, 7]

# Evaluasi setiap parameter dalam list_n
for n in list_n:

  # Latih model content based filtering
  cbf = CONTENT(cols['No Screen'])
  cbf.fit(df_data)

  precision_scores = []
  recall_scores = []
  f1_scores = []

  # Evaluasi setiap user
  for cellphone_id in df_label_con:

    # Evaluasi setiap relevant item
    for i in range(len(cellphone_id)):

      # Rekomendasi ponsel berdasarkan phone_id sebanyak n
      recom_id = cbf.recommendations(cellphone_id[i], n)
      # Ponsel yang relevan bagi pengguna
      relevants = [id for id in cellphone_id if id != cellphone_id[i]]
      # Hitung precision
      precision = precision_at_k(recom_id, relevants)
      # Hitung recall
      recall = recall_at_k(recom_id, relevants)
      # Hitung f1
      f1 = f1_at_k(recom_id, relevants)
      # Simpan precision
      precision_scores.append(precision)
      # Simpan recall
      recall_scores.append(recall)
      # Simpan f1
      f1_scores.append(f1)

  # Hasil rata-rata
  avg_precision = np.mean(precision_scores)
  avg_recall = np.mean(recall_scores)
  avg_f1 = np.mean(f1_scores)

  # Simpan hasil evaluasi untuk setiap parameter (kolom)
  results[n] = {
      'precision': avg_precision,
      'recall': avg_recall,
      'f1': avg_f1
  }

# Hasil evaluasi jumlah rekomendasi (n) pada content based filtering
sort_results = pd.DataFrame(results).T.apply(lambda x: round(x, 4)*100).sort_values('f1', ascending=False)
sort_results

# Grafik hasil evaluasi jumlah rekomendasi pada content based filtering
sort_results['f1'].sort_values().plot(kind='barh',
                                      title='Top-N Evaluation (Content Based Filtering)',
                                      xlim=(10, 20),
                                      xlabel='F1@K (%)',
                                      ylabel='N')
plt.show()

"""Berdasarkan hasil diatas tampak bahwa jumlah rekomendasi paling optimal pada content based filtering adalah 7. Nilai recall cenderung meningkat secara signifikan seiring dengan meningkatnya jumlah item yang direkomendasikan. Hal ini berarti jumlah rekomendasi yang relevan meningkat seiring dengan meningkatnya jumlah item yang direkomendasikan. Namun, disisi lain, jumlah item yang kurang relevan juga meningkat sehingga nilai presisi cenderung stagnan.

### **Rekomendasi**
"""

# Latih model content based filtering
cbf = CONTENT(cols['No Screen'])
cbf.fit(df_data)

name = 'Apple iPhone 13'
print('Ponsel acuan :', name)
id = name_to_id['Apple iPhone 13']
df_data[df_data['cellphone_id'] == id]

# Mendapatkan rekomendasi ponsel yang mirip
recom_id = cbf.recommendations(id, k=7)
print(f'Ponsel yang direkomendasikan dari acuan {name} :')
pd.DataFrame(recom_id).merge(df_data, on='cellphone_id')

"""## **Collaborative Filtering**

### **SVD**
"""

# Konversi dataframe menjadi matriks numpy
user_item_matrix = df_pivot_train.to_numpy()
# Dekomposisi matriks dengan k faktor latent
U, sigma, Vt = svds(user_item_matrix, k=10)
# Konversi sigma menjadi matriks diagonal
sigma = np.diag(sigma)
# Rekonstruksi matriks
pred_matrix = np.dot(np.dot(U, sigma), Vt)

# Konversi matriks hasil SVD menjadi dataframe
df_pred_rating = pd.DataFrame(pred_matrix, index=df_pivot_train.index, columns=df_pivot_train.columns)
df_pred_rating.head()

"""### **Evaluasi**"""

class COLLABORATIVE():
  def __init__(self, k):
    self.k = k

  def fit(self, df_user_item):
    # Copy dataframe
    self.df_user_item = df_user_item.copy()
    # Konversi dataframe menjadi matriks numpy
    user_item_matrix = self.df_user_item.to_numpy()
    # Dekomposisi matriks dengan k faktor latent
    U, sigma, Vt = svds(user_item_matrix, k=self.k)
    # Konversi sigma menjadi matriks diagonal
    sigma = np.diag(sigma)
    # Rekonstruksi matriks
    pred_matrix = np.dot(np.dot(U, sigma), Vt)
    # Konversi matriks hasil SVD menjadi dataframe
    self.df_pred_rating = pd.DataFrame(pred_matrix, index=df_user_item.index, columns=df_user_item.columns)

  def recommendations(self, user_id, num_recom=5):
    # Rating original dari user
    user_original_ratings = self.df_user_item.loc[user_id]
    # Prediksi rating untuk user
    user_predicted_ratings = self.df_pred_rating.loc[user_id]
    # Daftar item yang belum dirating oleh user
    items_not_rated = user_original_ratings[user_original_ratings == 0]
    # Prediksi rating untuk item yang belum dirating
    recommendations = user_predicted_ratings[items_not_rated.index]
    # Urutkan prediksi rating mulai dari yang tertinggi
    top_recommendations = recommendations.sort_values(ascending=False).head(num_recom)
    return top_recommendations

"""#### **Faktor Latent**"""

results = {}

# Jumlah faktor laten yang dievaluasi
list_k = [5, 10, 15, 20, 25]
# Jumlah rekomendasi
n = 5

# Evaluasi setiap nilai k
for k in list_k:

  # Latih model collaborative filtering menggunakan data masking
  clf = COLLABORATIVE(k=k)
  clf.fit(df_pivot_train)

  precision_scores = []
  recall_scores = []
  f1_scores = []

  # Evaluasi setiap user
  for user_id in df_label_col.index:

    # Rekomendasi ponsel berdasarkan user_id sebanyak n
    recom_id = clf.recommendations(user_id, num_recom=n).index
    # Ponsel yang relevan bagi pengguna
    relevants = df_label_col.loc[user_id]
    # Hitung precision
    precision = precision_at_k(recom_id, relevants)
    # Hitung recall
    recall = recall_at_k(recom_id, relevants)
    # Hitung f1
    f1 = f1_at_k(recom_id, relevants)
    # Simpan precision
    precision_scores.append(precision)
    # Simpan recall
    recall_scores.append(recall)
    # Simpan f1
    f1_scores.append(f1)

  # Hasil rata-rata
  avg_precision = np.mean(precision_scores)
  avg_recall = np.mean(recall_scores)
  avg_f1 = np.mean(f1_scores)

  # Simpan hasil evaluasi untuk setiap nilai k
  results[k] = {
      'precision': avg_precision,
      'recall': avg_recall,
      'f1': avg_f1
  }

# Hasil evaluasi jumlah faktor laten (k) pada collaborative filtering
sort_results = pd.DataFrame(results).T.apply(lambda x: round(x, 4)*100).sort_values('f1', ascending=False)
sort_results

# Grafik hasil evaluasi jumlah faktor laten (k) pada collaborative filtering
sort_results['f1'].sort_values().plot(kind='barh',
                                      title='K Latent Factor Evaluation (Collaborative Filtering)',
                                      xlim=(10, 16),
                                      xlabel='F1@K (%)',
                                      ylabel='K')
plt.show()

"""Berdasarkan hasil diatas tampak bahwa jumlah faktor laten yang paling optimal pada collaborative filtering adalah 15. Nilai tersebut berhasil menagkap pola preferensi pengguna secara optimal sehingga model memiliki kemampuan generalisasi yang tinggi tetapi tetap dapat menangkap pola yang penting. Faktor laten dengan jumlah 5 dan 10 belum mampu menagkap pola dari preferensi pengguna sehingga performanya cenderung rendah. Akibatnya, bisa jadi semua pengguna dengan preferensi sedikit berbeda akan menerima rekomendasi yang sama karena pola penting dari preferensi pengguna diabaikan. Faktor laten dengan jumlah 20 dan 25 menangkap terlalu banyak pola spesifik sehingga performanya tidak sebaik 15. Akibatnya, bisa jadi jika terdapat satu pengguna memberikan rating ekstrem pada item tertentu, model bisa memberikan rekomendasi bias untuk item itu.

#### **N Recommendation**
"""

results = {}

# Jumlah faktor laten
k = 15
# Jumlah rekomendasi (n) yang dievaluasi
list_n = [3, 4, 5, 6, 7]

# Evaluasi setiap nilai n dalam list_n
for n in list_n:

  # Latih model collaborative filtering menggunakan data masking
  clf = COLLABORATIVE(k=k)
  clf.fit(df_pivot_train)

  precision_scores = []
  recall_scores = []
  f1_scores = []

  # Evaluasi setiap user
  for user_id in df_label_col.index:

    # Rekomendasi ponsel berdasarkan user_id sebanyak n
    recom_id = clf.recommendations(user_id, num_recom=n).index
    # Ponsel yang relevan bagi pengguna
    relevants = df_label_col.loc[user_id]
    # Hitung precision
    precision = precision_at_k(recom_id, relevants)
    # Hitung recall
    recall = recall_at_k(recom_id, relevants)
    # Hitung f1
    f1 = f1_at_k(recom_id, relevants)
    # Simpan precision
    precision_scores.append(precision)
    # Simpan recall
    recall_scores.append(recall)
    # Simpan f1
    f1_scores.append(f1)

  # Hasil rata-rata
  avg_precision = np.mean(precision_scores)
  avg_recall = np.mean(recall_scores)
  avg_f1 = np.mean(f1_scores)

  # Simpan hasil evaluasi untuk setiap nilai n
  results[n] = {
      'precision': avg_precision,
      'recall': avg_recall,
      'f1': avg_f1
  }

# Hasil evaluasi jumlah rekomendasi (n) pada collaborative filtering
sort_results = pd.DataFrame(results).T.apply(lambda x: round(x, 4)*100).sort_values('f1', ascending=False)
sort_results

# Grafik hasil evaluasi jumlah rekomendasi (n) pada collaborative filtering
sort_results['f1'].sort_values().plot(kind='barh',
                                      title='Top-N Evaluation (Collaborative Filtering)',
                                      xlim=(10, 16),
                                      xlabel='F1-Score (%)',
                                      ylabel='N')
plt.show()

"""Berdasarkan hasil diatas tampak bahwa jumlah rekomendasi paling optimal pada collaborative filtering adalah 7. Nilai recall cenderung meningkat secara signifikan seiring dengan meningkatnya jumlah item yang direkomendasikan. Hal ini berarti jumlah rekomendasi yang relevan meningkat seiring dengan meningkatnya jumlah item yang direkomendasikan. Namun, disisi lain, jumlah item yang kurang relevan juga meningkat sehingga nilai presisi cenderung stagnan.

### **Rekomendasi**
"""

# Latih model collaborative filtering
clf = COLLABORATIVE(k=15)
clf.fit(df_pivot_train)

# ID user
user_id = df_pivot_train.index[1]

# Daftar ponsel yang dirating user
rating_user = df_rating_train[['cellphone_id', 'rating']][df_rating_train['user_id'] == user_id]
rating_user = rating_user.sort_values('rating', ascending=False)
print("Daftar ponsel yang dirating oleh user {}:".format(user_id))
rating_user.merge(df_data, on='cellphone_id')

# Rekomendasi untuk untuk user
recommendations = clf.recommendations(user_id, num_recom=7)
print(f'Rekomendasi untuk user {user_id} :')
pd.DataFrame(recommendations.index).merge(df_data, on='cellphone_id')